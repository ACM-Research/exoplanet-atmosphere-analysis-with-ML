{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54b7679a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\abhi\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\abhi\\anaconda3\\lib\\site-packages (from keras) (1.22.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\abhi\\anaconda3\\lib\\site-packages (from keras) (1.6.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\abhi\\anaconda3\\lib\\site-packages (from keras) (5.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\abhi\\anaconda3\\lib\\site-packages (from keras) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0d17618",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from scipy.stats import chisquare\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import time\n",
    "#from util import *\n",
    "#from ops import *\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "#from model import DCGAN\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "#np.set_printoptions(threshold='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0d64cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4256fcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.27030e-24 3.07616e-24 5.22539e-24 ... 1.32958e-21 1.33228e-21\n",
      "  1.35562e-21]\n",
      " [1.30267e-20 1.30677e-20 1.31157e-20 ... 3.81686e-20 3.81212e-20\n",
      "  3.80739e-20]\n",
      " [6.26322e-21 6.28294e-21 6.30602e-21 ... 1.83835e-20 1.83606e-20\n",
      "  1.83378e-20]\n",
      " ...\n",
      " [2.18046e-21 2.18672e-21 2.19415e-21 ... 4.09628e-21 4.09113e-21\n",
      "  4.08599e-21]\n",
      " [8.45625e-21 8.48075e-21 8.50976e-21 ... 1.65705e-20 1.65496e-20\n",
      "  1.65289e-20]\n",
      " [9.18122e-27 9.43401e-27 1.16085e-26 ... 4.02891e-22 4.05835e-22\n",
      "  4.14389e-22]]\n"
     ]
    }
   ],
   "source": [
    "#Setting up the dataset for ML\n",
    "# Text file data converted to integer data type\n",
    "x_data = np.loadtxt(\"dataset/x_dataTrain.txt\", dtype=float)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cb0361b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.44779525e-02 3.63563344e-01 3.93226327e-01 ... 3.84039400e-03\n",
      "  1.04632279e-03 8.05759487e-06]\n",
      " [4.16023007e-02 1.98831946e-01 3.19697149e-01 ... 1.58676888e-04\n",
      "  2.70865681e-04 6.98852685e-06]\n",
      " [3.25809662e-02 3.30184164e-01 2.25277622e-01 ... 3.64259914e-04\n",
      "  1.77938880e-05 5.15590766e-06]\n",
      " ...\n",
      " [6.00137545e-03 2.25446819e-01 2.36290974e-01 ... 3.41272939e-03\n",
      "  1.68012889e-04 8.55777941e-06]\n",
      " [9.61424398e-03 2.41019755e-01 4.74460251e-01 ... 2.28148605e-03\n",
      "  1.53862644e-04 3.59720563e-06]\n",
      " [1.48247666e-02 4.65782909e-01 3.00848722e-01 ... 5.70894991e-03\n",
      "  1.74632155e-04 1.76412088e-06]]\n"
     ]
    }
   ],
   "source": [
    "#Setting up the dataset for ML\n",
    "# Text file data converted to integer data type\n",
    "y_data = np.loadtxt(\"dataset/y_dataTrain.txt\", dtype=float)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "102d7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    0 : 'H2O',\n",
    "    1 : 'CO2',\n",
    "    2 : 'O2',\n",
    "    3 : 'N2',\n",
    "    4 : 'CH4',\n",
    "    5 : 'N2O',\n",
    "    6 : 'CO',\n",
    "    7 : 'O3',\n",
    "    8 : 'SO2',\n",
    "    9 : 'NH3',\n",
    "    10 : 'C2H6',\n",
    "    11 : 'NO2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ad84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(x_data)\n",
    "labels = pd.DataFrame(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15c9c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.rename(columns=feature_dict,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8793dcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H2O</th>\n",
       "      <th>CO2</th>\n",
       "      <th>O2</th>\n",
       "      <th>N2</th>\n",
       "      <th>CH4</th>\n",
       "      <th>N2O</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NH3</th>\n",
       "      <th>C2H6</th>\n",
       "      <th>NO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.054478</td>\n",
       "      <td>0.363563</td>\n",
       "      <td>0.393226</td>\n",
       "      <td>0.023477</td>\n",
       "      <td>0.128073</td>\n",
       "      <td>0.003160</td>\n",
       "      <td>0.020260</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.008082</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.041602</td>\n",
       "      <td>0.198832</td>\n",
       "      <td>0.319697</td>\n",
       "      <td>0.372766</td>\n",
       "      <td>0.045681</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>0.007645</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032581</td>\n",
       "      <td>0.330184</td>\n",
       "      <td>0.225278</td>\n",
       "      <td>0.367305</td>\n",
       "      <td>0.029888</td>\n",
       "      <td>0.009178</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047566</td>\n",
       "      <td>0.229283</td>\n",
       "      <td>0.522326</td>\n",
       "      <td>0.004332</td>\n",
       "      <td>0.143489</td>\n",
       "      <td>0.002901</td>\n",
       "      <td>0.019156</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.022003</td>\n",
       "      <td>0.007825</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.083559</td>\n",
       "      <td>0.197881</td>\n",
       "      <td>0.398276</td>\n",
       "      <td>0.222883</td>\n",
       "      <td>0.063702</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.016299</td>\n",
       "      <td>0.003417</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>0.032119</td>\n",
       "      <td>0.347432</td>\n",
       "      <td>0.359901</td>\n",
       "      <td>0.227611</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.006354</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.010497</td>\n",
       "      <td>0.180198</td>\n",
       "      <td>0.331853</td>\n",
       "      <td>0.423897</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.006691</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.003644</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.006001</td>\n",
       "      <td>0.225447</td>\n",
       "      <td>0.236291</td>\n",
       "      <td>0.465001</td>\n",
       "      <td>0.050457</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.003413</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.241020</td>\n",
       "      <td>0.474460</td>\n",
       "      <td>0.198173</td>\n",
       "      <td>0.049041</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.009849</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>0.014825</td>\n",
       "      <td>0.465783</td>\n",
       "      <td>0.300849</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.062158</td>\n",
       "      <td>0.012027</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.005709</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10003 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            H2O       CO2        O2        N2       CH4       N2O        CO  \\\n",
       "0      0.054478  0.363563  0.393226  0.023477  0.128073  0.003160  0.020260   \n",
       "1      0.041602  0.198832  0.319697  0.372766  0.045681  0.005339  0.007645   \n",
       "2      0.032581  0.330184  0.225278  0.367305  0.029888  0.009178  0.000921   \n",
       "3      0.047566  0.229283  0.522326  0.004332  0.143489  0.002901  0.019156   \n",
       "4      0.083559  0.197881  0.398276  0.222883  0.063702  0.004544  0.016299   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "9998   0.032119  0.347432  0.359901  0.227611  0.015221  0.004894  0.006354   \n",
       "9999   0.010497  0.180198  0.331853  0.423897  0.034217  0.006691  0.008135   \n",
       "10000  0.006001  0.225447  0.236291  0.465001  0.050457  0.000116  0.006873   \n",
       "10001  0.009614  0.241020  0.474460  0.198173  0.049041  0.007941  0.003215   \n",
       "10002  0.014825  0.465783  0.300849  0.125500  0.062158  0.012027  0.011423   \n",
       "\n",
       "             O3       SO2       NH3      C2H6       NO2  \n",
       "0      0.000787  0.008082  0.003840  0.001046  0.000008  \n",
       "1      0.002416  0.005585  0.000159  0.000271  0.000007  \n",
       "2      0.002042  0.002236  0.000364  0.000018  0.000005  \n",
       "3      0.001016  0.022003  0.007825  0.000087  0.000016  \n",
       "4      0.003417  0.006714  0.002307  0.000413  0.000005  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "9998   0.002510  0.002605  0.001277  0.000074  0.000002  \n",
       "9999   0.000428  0.003644  0.000265  0.000172  0.000002  \n",
       "10000  0.000623  0.005600  0.003413  0.000168  0.000009  \n",
       "10001  0.004247  0.009849  0.002281  0.000154  0.000004  \n",
       "10002  0.000431  0.001119  0.005709  0.000175  0.000002  \n",
       "\n",
       "[10003 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "395bc866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09ffd78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaledTrain = scaler.fit_transform(X_train)\n",
    "X_scaledTest = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "59e684a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = StandardScaler()\n",
    "y_scaledTrain = scaler.fit_transform(y_train)\n",
    "y_scaledTest = scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472b5769",
   "metadata": {},
   "source": [
    "# Testing Neural Network Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11497844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               560512    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 725,377\n",
      "Trainable params: 725,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Creating Neural Network model\n",
    "exoAtmos = Sequential()\n",
    "\n",
    "exoAtmos.add(Dense(128, kernel_initializer='normal',input_dim = X_scaledTrain.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "exoAtmos.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "exoAtmos.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "exoAtmos.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "exoAtmos.add(Dense(1, kernel_initializer='normal',activation='softmax'))\n",
    "\n",
    "# Compile the network :\n",
    "exoAtmos.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "exoAtmos.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19085d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/450\n",
      "168/168 [==============================] - 2s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 2/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 3/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 4/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 5/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 6/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 7/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 8/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 9/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 10/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 11/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 12/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 13/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 14/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 15/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 16/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 17/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 18/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 19/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 20/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 21/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 22/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 23/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 24/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 25/450\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.998 - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 26/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 27/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 28/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 29/450\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 30/450\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.995 - 1s 6ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 31/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 32/450\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 33/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 34/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 35/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 36/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 37/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 38/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 39/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 40/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 41/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 42/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 43/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 44/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 45/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 46/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 47/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 48/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 49/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 50/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 51/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 52/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 53/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 54/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 55/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 56/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 57/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 58/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 59/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 60/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 61/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 62/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 63/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 64/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 65/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 66/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 67/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 68/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 69/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 70/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 71/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 72/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 73/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 74/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 75/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 76/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 77/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 78/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 79/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 80/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 82/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 83/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 84/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 85/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 86/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 87/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 88/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 89/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 90/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 91/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 92/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 93/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 94/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 95/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 96/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 97/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 98/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 99/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 100/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 101/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 102/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 103/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 104/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 105/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 106/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 107/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 108/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 109/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 110/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 111/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 112/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 113/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 114/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 115/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 116/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 117/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 118/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 119/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 120/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 121/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 122/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 123/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 124/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 125/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 126/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 127/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 128/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 129/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 130/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 131/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 132/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 133/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 134/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 135/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 136/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 137/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 138/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 139/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 140/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 141/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 142/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 143/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 144/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 145/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 146/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 147/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 148/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 149/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 150/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 151/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 152/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 153/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 154/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 155/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 156/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 157/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 158/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 159/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 160/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 161/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 162/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 163/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 164/450\n",
      "168/168 [==============================] - ETA: 0s - loss: 1.999 - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 165/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 166/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 167/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 168/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 169/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 170/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 171/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 172/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 173/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 174/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 175/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 176/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 177/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 178/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 179/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 180/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 181/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 182/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 183/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 184/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 185/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 186/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 187/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 188/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 189/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 190/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 191/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 192/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 193/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 194/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 195/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 196/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 197/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 198/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 199/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 200/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 201/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 202/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 203/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 204/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 205/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 206/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 207/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 208/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 209/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 210/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 211/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 212/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 213/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 214/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 215/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 216/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 217/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 218/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 219/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 220/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 221/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 222/450\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 223/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 224/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 225/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 226/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 227/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 228/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 229/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 230/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 231/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 232/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 233/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 234/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 235/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 236/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 237/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 238/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 239/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 240/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 241/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 242/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 243/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 244/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 245/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 246/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 247/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 248/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 249/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 250/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 251/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 252/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 253/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 254/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 255/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 256/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 257/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 258/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 259/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 260/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 261/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 262/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 263/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 264/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 265/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 266/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 267/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 268/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 269/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 270/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 271/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 272/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 273/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 274/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 275/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 276/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 277/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 278/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 279/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 280/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 281/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 282/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 283/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 284/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 285/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 286/450\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 287/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 288/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 289/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 290/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 291/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 292/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 293/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 294/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 295/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 296/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 297/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 298/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 299/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 300/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 301/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 302/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 303/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 304/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 305/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 306/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 307/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 308/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 309/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 310/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 311/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 312/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 313/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 314/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 315/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 316/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 317/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 318/450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 319/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 320/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 321/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 322/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 323/450\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 324/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 325/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 326/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 327/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 328/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 329/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 330/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 331/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 332/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 333/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 334/450\n",
      "168/168 [==============================] - 1s 9ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 335/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 336/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 337/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 338/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 339/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 340/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 341/450\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 1.9971 - val_loss: 2.0117\n",
      "Epoch 342/450\n",
      "121/168 [====================>.........] - ETA: 0s - loss: 1.9922"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-95723b062837>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexoAtmos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_scaledTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_scaledTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m450\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training the model\n",
    "history = exoAtmos.fit(X_scaledTrain, y_scaledTrain, epochs=450, batch_size=32, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d793c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "exoAtmos.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ad6a17d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210/210 [==============================] - 1s 3ms/step - loss: 2.0000\n"
     ]
    }
   ],
   "source": [
    "test_loss = exoAtmos.evaluate(X_scaledTrain, y_scaledTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77d73eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0000009536743164"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7acf3bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67c2dfd",
   "metadata": {},
   "source": [
    "# Testing xGBoost Regressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
